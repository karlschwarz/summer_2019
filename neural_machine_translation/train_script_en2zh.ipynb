{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a99198d-6769-4932-8dd0-36e2e0cadf40",
   "metadata": {},
   "source": [
    "## Train Script of Seq2Seq RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a748f4db-735a-4801-bd3e-5af0b1228b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e867a2e9-90d6-41ee-b4bb-4d3515ebd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm.auto\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00da0b1-0fd1-48c8-bd59-10751438ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.decoder import DecoderRNN\n",
    "from models.encoder import EncoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3433cc-778c-4d8d-b4f1-4be47aa6f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240fe76-5c1d-4eae-9fed-82b948ef53ab",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5619adbb-02eb-48e4-880a-94454e5f1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12b818f-5f72-46ae-b4c6-a54dab424a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94b25bb-948d-43d9-ac1e-e496c39046d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=MAX_LENGTH, device=device):\n",
    "    encoder_hidden = encoder.initHidden(device)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0 \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            #topv, topi = decoder_output.topk(1)\n",
    "            #decoder_input = topi.squeeze().detach()\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c958fd-433f-4438-9bf2-58dc7b4a091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_train, epoch, device):\n",
    "    print(f\"\\nTrain Epoch: {epoch}.\")\n",
    "    loss_train, steps = 0.0, 0\n",
    "    for pair in tqdm.auto.tqdm(data_train):\n",
    "        input_tensor, target_tensor = tensorFromPair(input_lang, output_lang, pair, device)\n",
    "        loss = train(input_tensor, target_tensor, encoder, \n",
    "                     decoder, encoder_optimizer, decoder_optimizer, \n",
    "                     criterion)\n",
    "        loss_train += loss\n",
    "        steps += 1\n",
    "    loss_train_avg = loss_train / steps\n",
    "    template_print = f\"Epoch(train): {epoch} Loss_train: {loss_train_avg:.3f}.\"\n",
    "    print(template_print)\n",
    "    return loss_train, loss_train_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f807d8-ed13-4717-8158-489cfabbd474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation(encoder, decoder, sentence, sentence_target, max_length=MAX_LENGTH, device=device):\n",
    "    with torch.no_grad():\n",
    "        sentence_list = list(jieba.cut(sentence_target, cut_all=False))\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence, device)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden(device=device)\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], \n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        max_n = min(4, len(decoded_words[:-1]), len(sentence_list))\n",
    "        if max_n == 0:\n",
    "            bleu = 0\n",
    "        else:\n",
    "            weights = (1.0 / max_n, ) * max_n\n",
    "            bleu = data.metrics.bleu_score([decoded_words[:-1]], [[sentence_list]], max_n=max_n, weights=weights)\n",
    "        return decoded_words, bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06403f66-352c-4321-af96-eff8e3362b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(encoder, decoder, n=10):\n",
    "    bleu_acm = 0.0\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, bleu_i = evaluation(encoder, decoder, pair[0], pair[1])\n",
    "        bleu_acm += bleu_i\n",
    "    bleu_avg = bleu_acm / n\n",
    "    print(f\"avg bleu is: {bleu_avg:.3f}.\")\n",
    "    return bleu_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd957fe7-e0f7-4d3e-b94a-64ce4535c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    bleu_acm = 0.0\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, bleu_i = evaluation(encoder, decoder, pair[0], pair[1])\n",
    "        bleu_acm += bleu_i\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "    bleu_avg = bleu_acm / n\n",
    "    print(f\"avg bleu is: {bleu_avg:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ce5ae-023c-496a-a666-2b8b63e11583",
   "metadata": {},
   "source": [
    "----------------------\n",
    "### Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e000e1c-7330-41a3-a9fb-7eb66ba2afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 29458 sentence pairs\n",
      "Counting words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.607 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "en 7367\n",
      "zh 16261\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareDataMand('en', 'zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad08b09-e472-4f0b-b4ad-91b99fc94587",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef53dcf5-657a-4f79-9f99-a2e42dbc1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9959f226-9091-44a8-acab-c77f848d1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39ab81c-b770-4a83-aef2-e26e4c708fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=1e-3)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99fbded5-cd63-4beb-9aae-c92364fb4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "581e08e9-7ff6-4e6c-9fe5-1e2f38b352b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d8fc31-4e26-499d-a41c-249481a1e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cbcfe25-c8e4-4754-b9a9-92ec26ea6ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 5.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92be1013ba14de894302fa585922a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     loss_train, loss_train_avg \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     bleu_avg_i \u001b[38;5;241m=\u001b[39m evaluate_bleu(encoder, decoder, \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m      9\u001b[0m     loss_train_avg_list\u001b[38;5;241m.\u001b[39mappend(loss_train_avg)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data_train, epoch, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mauto\u001b[38;5;241m.\u001b[39mtqdm(data_train):\n\u001b[1;32m      5\u001b[0m     input_tensor, target_tensor \u001b[38;5;241m=\u001b[39m tensorFromPair(input_lang, output_lang, pair, device)\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     10\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m#topv, topi = decoder_output.topk(1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m#decoder_input = topi.squeeze().detach()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_length):\n\u001b[0;32m---> 33\u001b[0m         decoder_output, decoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         topv, topi \u001b[38;5;241m=\u001b[39m decoder_output\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m         decoder_input \u001b[38;5;241m=\u001b[39m topi\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/mylab/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tnn_torch/seq2seq/model/decoder.py:20\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(output)\n\u001b[1;32m     19\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(output, hidden)\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[0;32m~/anaconda3/envs/mylab/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mylab/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_train_avg_list, bleu_score_list = [], []\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(1, 2, 1);\n",
    "ax2 = fig.add_subplot(1, 2, 2);\n",
    "ax1.set_xlabel('test');\n",
    "for i in range(1, num_epochs+1):\n",
    "    loss_train, loss_train_avg = train_epoch(pairs, i, device)\n",
    "    bleu_avg_i = evaluate_bleu(encoder, decoder, 500)\n",
    "    loss_train_avg_list.append(loss_train_avg)\n",
    "    bleu_score_list.append(bleu_avg_i)\n",
    "    x1 = np.arange(0, i);\n",
    "    \n",
    "    x2 = np.arange(0, i);\n",
    "    \n",
    "    ax1.set_xlim(0, i)\n",
    "    ax2.set_xlim(0, i)\n",
    "    \n",
    "    ax1.cla()\n",
    "    ax1.plot(x1, loss_train_avg_list)\n",
    "    \n",
    "    ax2.cla()\n",
    "    ax2.plot(x2, bleu_score_list)\n",
    "    \n",
    "    display(fig)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    plt.pause(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f3aaf-f79f-42fa-99ed-c9e74a017369",
   "metadata": {
    "tags": []
   },
   "source": [
    "plt.plo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805a3a4-4b2d-457e-a81f-4c5b12d1b5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
