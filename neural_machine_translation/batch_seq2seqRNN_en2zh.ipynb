{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e721b0-6933-4898-9bd3-1e49a3ba656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bcdb9b-b8a0-4433-9e19-cb987290fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import jieba\n",
    "import tqdm.auto\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4cec5e-aa48-4c0a-bdb7-0816519de8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_batch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769c51d7-f47f-417b-8453-730f2218923d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.decoder_batch import DecoderRNN\n",
    "from models.encoder_batch import EncoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa88267-6f99-4c3f-bb19-79291e53e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_create(batch_size, maxlen, output_size, pad_token, device, train=True):\n",
    "    if train:\n",
    "        mask = torch.ones((batch_size, maxlen, output_size), device=device)\n",
    "        mask[:, :, pad_token] = 0\n",
    "    else:\n",
    "        mask = torch.ones((batch_size, 1, output_size), device=device)\n",
    "        mask[:, :, pad_token] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ed1a783e-5765-4cdf-a1e9-b7981a406735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, mask_train, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, device):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "        \n",
    "    batch_size = input_tensor.size(0)\n",
    "\n",
    "    loss = 0 \n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    \n",
    "    decoder_inputs = torch.zeros_like(target_tensor, dtype=torch.long, device=device)\n",
    "    decoder_inputs[:, 0] = SOS_token\n",
    "    decoder_inputs[:, 1:] = target_tensor[:, :-1]\n",
    "    decoder_input = decoder_inputs[:, 0].view(-1, 1)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # always use_teacher_forcing:\n",
    "    if True:\n",
    "        decoder_outputs, decoder_hidden = decoder(decoder_inputs, decoder_hidden)\n",
    "        decoder_outputs.masked_fill(mask_train == 0, -1e10)\n",
    "        decoder_outputs = torch.nn.LogSoftmax(dim=-1)(decoder_outputs)\n",
    "        #print(decoder_outputs.shape)\n",
    "        loss = criterion(decoder_outputs.permute(0, 2, 1), target_tensor)\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            loss += criterion(decoder_output.view(-1, output_size), target_tensor[:, di])\n",
    "            #if decoder_input.item() == EOS_token:\n",
    "            #    break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c2f92643-e31e-4860-bb94-ac1b701731d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, epoch, device):\n",
    "    print(f\"\\nTrain Epoch: {epoch}.\")\n",
    "    loss_train, steps = 0.0, 0\n",
    "    for input_sample, target_sample in train_loader:\n",
    "        input_sample, target_sample = input_sample.to(device), target_sample.to(device)\n",
    "        loss = train(input_sample, target_sample, mask_train, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, device)\n",
    "        loss_train += loss\n",
    "        steps += 1\n",
    "    loss_train_avg = loss_train / steps\n",
    "    template_print = f\"Epoch(train): {epoch} Loss_train: {loss_train_avg:.3f}.\"\n",
    "    print(template_print)\n",
    "    return loss_train_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36e5f390-9468-4648-aa09-095dde6a14d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "142821fd-f7ec-45ce-8c4d-2aca7df77294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_tensor, target_tensor, mask_eval, encoder, decoder, device, num_pairs_to_show):\n",
    "    with torch.no_grad():        \n",
    "        batch_size, max_length = input_tensor.shape[0], input_tensor.shape[1]\n",
    "        _, encoder_hidden = encoder(input_tensor)\n",
    "        ### first input of the sequence is SOS\n",
    "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.int)\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_index_list = []\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            decoder_output.masked_fill(mask_eval == 0, -1e5)\n",
    "            decoder_output = torch.nn.LogSoftmax(dim=-1)(decoder_output)\n",
    "            decoder_output = decoder_output.squeeze(1)\n",
    "            #print(decoder_output.shape)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoded_index_list.append(topi.squeeze(-1).cpu().numpy())\n",
    "            decoder_input = topi.detach()\n",
    "        decoded_index_array = np.array(decoded_index_list).transpose()\n",
    "        bleu_accum = 0\n",
    "        decoded_pairs = []\n",
    "        for i in range(batch_size):\n",
    "            input_word_list = []\n",
    "            output_word_list = []\n",
    "            target_word_list = []\n",
    "            decoded_index = decoded_index_array[i].tolist()\n",
    "            for word_index in input_tensor[i].tolist():\n",
    "                if word_index == EOS_token:\n",
    "                    break\n",
    "                else:\n",
    "                    input_word_list.append(input_lang.index2word[word_index])\n",
    "            for word_index in decoded_index:\n",
    "                if word_index == EOS_token:\n",
    "                    break\n",
    "                else:\n",
    "                    output_word_list.append(output_lang.index2word[word_index])\n",
    "            for word_index in target_tensor[i].tolist():\n",
    "                if word_index == EOS_token:\n",
    "                    break\n",
    "                else:\n",
    "                    target_word_list.append(output_lang.index2word[word_index])\n",
    "            max_n = min(4, len(output_word_list), len(target_word_list))\n",
    "            if max_n == 0:\n",
    "                bleu = 0\n",
    "            else:\n",
    "                weights = (1.0 / max_n, ) * max_n\n",
    "                try:\n",
    "                    bleu = data.metrics.bleu_score([output_word_list], [[target_word_list]], max_n=max_n, weights=weights)\n",
    "                    decoded_pairs.append([input_word_list, output_word_list, target_word_list])\n",
    "                except IndexError:\n",
    "                    bleu = 0\n",
    "                    print(f'decoded_words: {output_word_list}.')\n",
    "                    print(f'sentence_list: {target_word_list}.')\n",
    "                    print(f'max_n: {max_n}.')\n",
    "            bleu_accum += bleu\n",
    "        bleu_avg = bleu_accum / batch_size\n",
    "        if len(decoded_pairs) <= num_pairs_to_show:\n",
    "            decoded_pairs_sampled = decoded_pairs\n",
    "        else:\n",
    "            decoded_pairs_sampled = random.sample(decoded_pairs, num_pairs_to_show)\n",
    "        return bleu_avg, decoded_pairs_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "77053f08-3873-4933-b9e2-978744220829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(test_loader, epoch_id, device):\n",
    "    print(f\"\\nTest Epoch: {epoch_id}.\")\n",
    "    bleu_accum, steps = 0.0, 0\n",
    "    for input_sample, target_sample in test_loader:\n",
    "        input_sample, target_sample = input_sample.to(device), target_sample.to(device)\n",
    "        bleu_avg, decoded_pairs = evaluate(input_sample, target_sample, mask_eval, encoder, decoder, device, 10)\n",
    "        bleu_accum += bleu_avg\n",
    "        steps += 1\n",
    "    bleu_avg_epoch = bleu_accum / steps\n",
    "    template_print = f\"Epoch(test): {epoch_id} Bleu_test: {bleu_avg_epoch:.3f}.\"\n",
    "    print(template_print)\n",
    "    for pair in decoded_pairs:\n",
    "        input_pair, decoded_pair, target_pair = pair\n",
    "        input_sentence = ' '.join(input_pair)\n",
    "        decoded_sentence = ''.join(decoded_pair)\n",
    "        target_sentence = ''.join(target_pair)\n",
    "        print('----------------------------------')\n",
    "        print('<', input_sentence)\n",
    "        print('=', target_sentence)\n",
    "        print('>', decoded_sentence)\n",
    "    return bleu_avg_epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d5de0ac4-ccf1-4692-ab24-0113d3e4d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_avg, decoded_pairs = evaluate(input_sample, target_sample, mask_eval, encoder, decoder, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47b2f1-574c-497e-ad2d-53be96f23622",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_epoch(enzh_loader_test, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "334a1cdc-80e4-4a24-aad4-f917f5b56343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, data_valid, n=100):\n",
    "    bleu_acm = 0.0\n",
    "    for i in range(n):\n",
    "        pair = random.choice(data_valid)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, bleu_i = evaluate(encoder, decoder, pair[0], pair[1])\n",
    "        bleu_acm += bleu_i\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "    bleu_avg = bleu_acm / n\n",
    "    print(f\"avg bleu is: {bleu_avg:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dae6d6-543f-42eb-859e-66bc60a56e97",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0315f02a-c878-4956-910c-292923b062e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a960aa6a-c0b0-4d19-8c2b-a4e7cfbf4aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 29458 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 7368\n",
      "zh 16262\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/cmn.txt'\n",
    "input_lang, output_lang, pairs = prepareDataMand(file_path, 'en', 'zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8715c04-4479-437a-83f1-79d7bace7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_input = input_lang.max_length()\n",
    "max_length_output = output_lang.max_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "affc27e6-cff4-41bd-a9fc-0e3e56538029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e8127ba0-9d52-4644-9a03-e8e4b64f2709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ea390ea5-110b-4396-a196-e9c6f54f4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "maxlen = 35\n",
    "device = torch.device('cuda:4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b047d4c-eda8-493a-b7a6-1616335879ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 29458 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 7368\n",
      "zh 16262\n",
      "26513 samples in training dataset.\n",
      "2945 samples in valid dataset.\n"
     ]
    }
   ],
   "source": [
    "enzh_loader_train, enzh_loader_test, _ = enzh_loader(file_path, batch_size, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6b6fbd0c-335a-48d7-b7ed-bfd27a286ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample, target_sample = next(iter(enzh_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4e5df10f-581a-40c7-987f-4b033c9f720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample, target_sample = input_sample.to(device), target_sample.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f6fd2045-f492-43a8-91d5-d5e366f305dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(input_sample, target_sample, mask_eval, encoder, decoder, device, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4a999483-b561-4644-9815-579cdbfdac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4ed0ba28-db9a-491e-b02a-76dc29e05ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = mask_create(batch_size, maxlen, output_size, PAD_token, device, train=True)\n",
    "mask_eval = mask_create(batch_size, maxlen, output_size, PAD_token, device, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c1a18246-535c-44ff-bb24-e59cf69d437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "18c090bc-937a-477c-94fc-34c1c7c05bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=4e-1)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=4e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8da7a6c5-6598-4ff2-85a6-694f3ae887cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.NLLLoss(ignore_index=PAD_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "23982092-7c80-4bd7-a9a7-993ac471c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a1f90631-d5ed-436e-af5c-97c8354f449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852acc26-f006-4fc1-b205-bea84086ab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 260.\n",
      "Epoch(train): 260 Loss_train: 0.120.\n",
      "\n",
      "Test Epoch: 260.\n",
      "Epoch(test): 260 Bleu_test: 0.045.\n",
      "----------------------------------\n",
      "< everyone is talking about tom \n",
      "= 人人都在說湯姆\n",
      "> 所有人都非常感謝你\n",
      "----------------------------------\n",
      "< ive always trusted your judgment \n",
      "= 我一直都相信你的判断\n",
      "> 我完全不负过你\n",
      "----------------------------------\n",
      "< please forgive me for being late \n",
      "= 請原諒我遲到\n",
      "> 请我帮我\n",
      "----------------------------------\n",
      "< i dont like science \n",
      "= 我不喜欢科学\n",
      "> 我不喜欢狗\n",
      "----------------------------------\n",
      "< tom said that he was curious \n",
      "= 汤姆说他很好奇\n",
      "> 汤姆说他很好奇\n",
      "----------------------------------\n",
      "< tom didnt understand what the teacher said \n",
      "= 汤姆没明白老师说了什么\n",
      "> 湯姆說沒有人說謊\n",
      "----------------------------------\n",
      "< the monkey fell from the tree \n",
      "= 猴子從樹上掉了下來\n",
      "> 天空从的房子爬上\n",
      "----------------------------------\n",
      "< i have to win \n",
      "= 我必须赢\n",
      "> 我需要一名醫生\n",
      "----------------------------------\n",
      "< i asked him for a favor \n",
      "= 我請他幫忙\n",
      "> 我向他求助\n",
      "----------------------------------\n",
      "< my parents have gone to the airport to see my uncle off \n",
      "= 我父母去机场送我叔叔了\n",
      "> 妈妈我妈妈就把所有的书\n",
      "\n",
      "Train Epoch: 261.\n",
      "Epoch(train): 261 Loss_train: 0.106.\n",
      "\n",
      "Test Epoch: 261.\n",
      "Epoch(test): 261 Bleu_test: 0.041.\n",
      "----------------------------------\n",
      "< he prays several times a day \n",
      "= 他每天祈祷好几次\n",
      "> 湯姆以前很早起床\n",
      "----------------------------------\n",
      "< if it had not been for civil war  they would be wealthy now \n",
      "= 要不是当初有内战他们现在应该挺富裕了\n",
      "> 如果没有太阳就那么做的工作了\n",
      "----------------------------------\n",
      "< wheres the hammer \n",
      "= 锤子在哪里\n",
      "> 下一列了\n",
      "----------------------------------\n",
      "< are you free this afternoon \n",
      "= 你们今天下午有空吗\n",
      "> 早饭人生在日本多久\n",
      "----------------------------------\n",
      "< my home is your home \n",
      "= 我家就是你家\n",
      "> 我的血压很低\n",
      "----------------------------------\n",
      "< wheres the door \n",
      "= 门在哪里\n",
      "> 自助餐厅在哪里\n",
      "----------------------------------\n",
      "< there is something wrong with my car \n",
      "= 我的車子有問題\n",
      "> 我的車子故障了\n",
      "----------------------------------\n",
      "< he sometimes goes to tokyo on business \n",
      "= 他时不时地因为工作去东京\n",
      "> 他在這裡工作之前了但什么也没办法入睡\n",
      "----------------------------------\n",
      "< i spoke to tom last night \n",
      "= 昨晚我和汤姆说话了\n",
      "> 我昨晚没睡了\n",
      "----------------------------------\n",
      "< im as old as he is \n",
      "= 我和他同年\n",
      "> 我不認為他的手錶\n",
      "\n",
      "Train Epoch: 262.\n",
      "Epoch(train): 262 Loss_train: 0.097.\n",
      "\n",
      "Test Epoch: 262.\n",
      "Epoch(test): 262 Bleu_test: 0.043.\n",
      "----------------------------------\n",
      "< i am a translator \n",
      "= 我是个翻译\n",
      "> 我是个翻译\n",
      "----------------------------------\n",
      "< try it again \n",
      "= 再试一下\n",
      "> 再試一次\n",
      "----------------------------------\n",
      "< are you interested in foreign languages \n",
      "= 你们对外语感兴趣吗\n",
      "> 你对复合碳水化合物这件好\n",
      "----------------------------------\n",
      "< if you stay at a big hotel  you can use their swimming pool \n",
      "= 若你住到的是大的旅館就能使用他們的游泳池\n",
      "> 待你在游泳但什么时候学会游泳吧\n",
      "----------------------------------\n",
      "< i want to see a movie \n",
      "= 我要去看電影\n",
      "> 我要看病来\n",
      "----------------------------------\n",
      "< i cant put up with this smell \n",
      "= 我不能忍受這種氣味\n",
      "> 我今天早上寫信給\n",
      "----------------------------------\n",
      "< he likes watching tv \n",
      "= 他喜歡看電視\n",
      "> 他跑得非常快\n",
      "----------------------------------\n",
      "< you have to finish this by 2:30 \n",
      "= 你必须在两点半之前完成它\n",
      "> 你必须改掉到东京\n",
      "----------------------------------\n",
      "< tom put a lot of honey on his toast \n",
      "= 汤姆往他的吐司上放了许多蜂蜜\n",
      "> 汤姆往他的面包片上放了许多蜂蜜\n",
      "----------------------------------\n",
      "< she didnt like living in the city \n",
      "= 她不喜歡住在城市裡\n",
      "> 她非常想外出\n",
      "\n",
      "Train Epoch: 263.\n",
      "Epoch(train): 263 Loss_train: 0.092.\n",
      "\n",
      "Test Epoch: 263.\n",
      "Epoch(test): 263 Bleu_test: 0.041.\n",
      "----------------------------------\n",
      "< i know youre learning french at school \n",
      "= 我知道你在學校學法語\n",
      "> 我知道你在學校學法語\n",
      "----------------------------------\n",
      "< how did you come by this money \n",
      "= 用這些錢你怎麼來的 \n",
      "> 这本书一点钱\n",
      "----------------------------------\n",
      "< he has dozens of english books \n",
      "= 他有幾十本的英文書籍\n",
      "> 他酷爱足球\n",
      "----------------------------------\n",
      "< i know how to set a trap \n",
      "= 我知道怎么设陷阱\n",
      "> 我知道为什么天准备好\n",
      "----------------------------------\n",
      "< tom should be worried \n",
      "= tom應該要擔心\n",
      "> 湯姆擔心也不存在\n",
      "----------------------------------\n",
      "< i ran as fast as possible \n",
      "= 我跑得尽可能快\n",
      "> 我盡可能地跑快一點\n",
      "----------------------------------\n",
      "< all that i know is that he gave up the plan \n",
      "= 我所知道的是他放棄了這個計劃\n",
      "> 我很想死的\n",
      "----------------------------------\n",
      "< i had trouble unwrapping the package \n",
      "= 我在打开包装时遇到了麻烦\n",
      "> 我不在乎将来吃晚餐\n",
      "----------------------------------\n",
      "< i got a good grade in english \n",
      "= 我的英文成績很好\n",
      "> 我的英文得到了好成績\n",
      "----------------------------------\n",
      "< are you going to use this \n",
      "= 你要用這東西嗎\n",
      "> 你要用這東西嗎\n",
      "\n",
      "Train Epoch: 264.\n",
      "Epoch(train): 264 Loss_train: 0.087.\n",
      "\n",
      "Test Epoch: 264.\n",
      "Epoch(test): 264 Bleu_test: 0.041.\n",
      "----------------------------------\n",
      "< tom broke into marys hotel room \n",
      "= 湯姆闖進了瑪麗的酒店房間\n",
      "> 汤姆把钱包放在桌子底下\n",
      "----------------------------------\n",
      "< wheres the closest pharmacy \n",
      "= 最近的藥房在哪裡 \n",
      "> 在出口的開始在哪裡\n",
      "----------------------------------\n",
      "< clean up the room \n",
      "= 收拾房间\n",
      "> 我把狗跑走了\n",
      "----------------------------------\n",
      "< tom isnt nearly as smart as he thinks he is \n",
      "= 汤姆并不像他自认为的那么聪明\n",
      "> 汤姆说他很确定他就做到和他结婚了\n",
      "----------------------------------\n",
      "< is there enough food for everyone \n",
      "= 有足夠的食物給大家嗎\n",
      "> 你應該準時 \n",
      "----------------------------------\n",
      "< tom took friday off \n",
      "= 汤姆星期五闲着\n",
      "> 汤姆把它留在了\n",
      "----------------------------------\n",
      "< the plane is about to land \n",
      "= 飛機就要降落了\n",
      "> 小偷不一起出去\n",
      "----------------------------------\n",
      "< i won \n",
      "= 我赢了\n",
      "> 我睡了\n",
      "----------------------------------\n",
      "< ive won first prize \n",
      "= 我贏得了第一名\n",
      "> 我得卖\n",
      "----------------------------------\n",
      "< tom showed interest in the plan \n",
      "= 湯姆對這個計劃有興趣\n",
      "> 湯姆看來在同一一個有趣的工作\n",
      "\n",
      "Train Epoch: 265.\n",
      "Epoch(train): 265 Loss_train: 0.083.\n",
      "\n",
      "Test Epoch: 265.\n",
      "Epoch(test): 265 Bleu_test: 0.040.\n",
      "----------------------------------\n",
      "< are you busy \n",
      "= 您在忙吗\n",
      "> 你忙嗎\n",
      "----------------------------------\n",
      "< were not in boston \n",
      "= 我們不在波士頓\n",
      "> 我最近在波士吃过苹果\n",
      "----------------------------------\n",
      "< we promised to stand by him in case of trouble \n",
      "= 我們承諾萬一他有麻煩的時候支持他\n",
      "> 我们所有人在公园的时候几乎准备公交车了下来\n",
      "----------------------------------\n",
      "< if you dont give it back to him  hell be angry \n",
      "= 要是你不还给他他就会生气\n",
      "> 如果了那持续到它不见就他它\n",
      "----------------------------------\n",
      "< she cursed loudly \n",
      "= 她大声诅咒\n",
      "> 她爱她的書\n",
      "----------------------------------\n",
      "< i cant go  nor do i want to \n",
      "= 我去不了也不想去\n",
      "> 我不記得我的提議\n",
      "----------------------------------\n",
      "< john is two years older than me \n",
      "= john比我大两岁\n",
      "> john比我大两岁\n",
      "----------------------------------\n",
      "< are you sure tom will come back \n",
      "= 你確定湯姆會回來嗎\n",
      "> 你希望汤姆能解决吗\n",
      "----------------------------------\n",
      "< i have no idea of whats going on in there \n",
      "= 我不清楚這裡發生了什麼事\n",
      "> 我不确定 现在应该\n",
      "----------------------------------\n",
      "< i think youll have very little difficulty in getting a drivers license \n",
      "= 我想你要拿到驾照根本不难\n",
      "> 我想您要拿到驾照根本不难\n",
      "\n",
      "Train Epoch: 266.\n",
      "Epoch(train): 266 Loss_train: 0.080.\n",
      "\n",
      "Test Epoch: 266.\n",
      "Epoch(test): 266 Bleu_test: 0.043.\n",
      "----------------------------------\n",
      "< toms comment was inappropriate \n",
      "= 湯姆的評論不合適\n",
      "> 湯姆決定有點快\n",
      "----------------------------------\n",
      "< im now rich enough to afford to get anything i want \n",
      "= 我現在有錢能想買就買\n",
      "> 我買了那輛車\n",
      "----------------------------------\n",
      "< who are you working for \n",
      "= 你為誰工作\n",
      "> 你在閒暇我們的做了\n",
      "----------------------------------\n",
      "< im looking for work \n",
      "= 我在找工作\n",
      "> 我在找工作\n",
      "----------------------------------\n",
      "< i cant remember \n",
      "= 我記不得了\n",
      "> 我想不起來\n",
      "----------------------------------\n",
      "< itll cost around ten thousand yen \n",
      "= 它將花費大約10 000日元\n",
      "> 這將花費約10000日元\n",
      "----------------------------------\n",
      "< you were hurt  werent you \n",
      "= 你受伤了不是吗\n",
      "> 你受伤了是吧\n",
      "----------------------------------\n",
      "< i missed you \n",
      "= 我想你\n",
      "> 我想念你\n",
      "----------------------------------\n",
      "< tom talked about his school \n",
      "= 湯姆談了他的學校\n",
      "> 湯姆和他一起玩得很開心\n",
      "----------------------------------\n",
      "< he was able to solve the problem \n",
      "= 他能解決這個問題\n",
      "> 除了他今晚都感觉\n",
      "\n",
      "Train Epoch: 267.\n",
      "Epoch(train): 267 Loss_train: 0.078.\n",
      "\n",
      "Test Epoch: 267.\n",
      "Epoch(test): 267 Bleu_test: 0.041.\n",
      "----------------------------------\n",
      "< open your eyes  please \n",
      "= 请睁开眼睛\n",
      "> 修理点\n",
      "----------------------------------\n",
      "< today is my birthday \n",
      "= 今天是我的生日\n",
      "> 今天是我生日\n",
      "----------------------------------\n",
      "< when did she promise to meet him \n",
      "= 她答应几时见他\n",
      "> 她什麼時候不要\n",
      "----------------------------------\n",
      "< i got off at the wrong station \n",
      "= 我在錯誤的車站下車\n",
      "> 我下錯了車站\n",
      "----------------------------------\n",
      "< tom broke up with mary \n",
      "= 汤姆和玛丽分手了\n",
      "> 汤姆烤了玛丽\n",
      "----------------------------------\n",
      "< there are many ways to improve ones life \n",
      "= 有很多的方法可以改善人的一生\n",
      "> 倫敦有很多事要\n",
      "----------------------------------\n",
      "< we have to pull the weeds \n",
      "= 我們必須拔除雜草\n",
      "> 我們必須有三個到\n",
      "----------------------------------\n",
      "< she filled the glass with wine \n",
      "= 她在玻璃杯裡裝滿了酒\n",
      "> 她和铅笔在一起\n",
      "----------------------------------\n",
      "< help me lift the package \n",
      "= 帮我抬包裹\n",
      "> 帮我看一下酒單\n",
      "----------------------------------\n",
      "< how about a drink after the game \n",
      "= 比賽結束後喝一杯怎麼樣\n",
      "> 请问鸡蛋刀\n",
      "\n",
      "Train Epoch: 268.\n"
     ]
    }
   ],
   "source": [
    "loss_train_avg_list, bleu_score_list = [], []\n",
    "#fig = plt.figure(figsize=(12, 4))\n",
    "#ax1 = fig.add_subplot(1, 2, 1);\n",
    "#ax2 = fig.add_subplot(1, 2, 2);\n",
    "for i in trange(1, num_epochs+1):\n",
    "    #display(fig)\n",
    "    #x1 = np.arange(0, i-1);\n",
    "    #x2 = np.arange(0, i-1);\n",
    "    \n",
    "    #ax1.set_xlim(0, i)\n",
    "    #ax2.set_xlim(0, i)\n",
    "    #ax1.set_xlabel('Epoch')\n",
    "    #ax2.set_xlabel('Epoch')\n",
    "    \n",
    "    #ax1.cla()\n",
    "    #ax1.plot(x1, loss_train_avg_list)\n",
    "    \n",
    "    #ax2.cla()\n",
    "    #ax2.plot(x2, bleu_score_list)\n",
    "    if i % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "    loss_train_avg = train_epoch(enzh_loader_train, i, device)\n",
    "    bleu_avg_i = evaluate_epoch(enzh_loader_test, i, device)\n",
    "    loss_train_avg_list.append(loss_train_avg)\n",
    "    bleu_score_list.append(bleu_avg_i)\n",
    "\n",
    "    #plt.pause(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8163a-81a2-457d-9be3-688f05b393a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39fa02-f401-479c-adb9-38504021bdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
